
@article{maaloe_biva_nodate,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, ﬂow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classiﬁcation tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	language = {en},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	pages = {12},
	file = {BIVA_camera_ready_appendix_v2.pdf:/Users/mag/Zotero/storage/F9XHSNDZ/BIVA_camera_ready_appendix_v2.pdf:application/pdf;Maaløe et al. - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:/Users/mag/Zotero/storage/GCR945VW/Maaløe et al. - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:application/pdf},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2020-12-03},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = aug,
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/7CVKYWTV/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WEU4N3RE/1606.html:text/html},
}

@inproceedings{fraccaro_generative_2018,
	title = {Generative {Temporal} {Models} with {Spatial} {Memory} for {Partially} {Observed} {Environments}},
	url = {http://proceedings.mlr.press/v80/fraccaro18a.html},
	abstract = {In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent’s representations during training or vi...},
	language = {en},
	urldate = {2021-02-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fraccaro, Marco and Rezende, Danilo and Zwols, Yori and Pritzel, Alexander and Eslami, S. M. Ali and Viola, Fabio},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1549--1558},
	file = {Snapshot:/Users/mag/Zotero/storage/VTFL7F58/fraccaro18a.html:text/html;Full Text PDF:/Users/mag/Zotero/storage/8MZXF9TZ/Fraccaro et al. - 2018 - Generative Temporal Models with Spatial Memory for.pdf:application/pdf},
}

@article{gemici_generative_2017,
	title = {Generative {Temporal} {Models} with {Memory}},
	url = {http://arxiv.org/abs/1702.04649},
	abstract = {We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.},
	urldate = {2021-02-04},
	journal = {arXiv:1702.04649 [cs, stat]},
	author = {Gemici, Mevlana and Hung, Chia-Chun and Santoro, Adam and Wayne, Greg and Mohamed, Shakir and Rezende, Danilo J. and Amos, David and Lillicrap, Timothy},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04649},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/LV2BVV5F/Gemici et al. - 2017 - Generative Temporal Models with Memory.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/5ZYYIMHG/1702.html:text/html},
}

@article{sonderby_ladder_2016,
	title = {Ladder {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1602.02282},
	abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
	urldate = {2021-02-07},
	journal = {arXiv:1602.02282 [cs, stat]},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	month = may,
	year = {2016},
	note = {arXiv: 1602.02282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/JAEZIP5W/Sønderby et al. - 2016 - Ladder Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/62GGTJTI/1602.html:text/html},
}

@inproceedings{cohen_how_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {How to {Train} {Deep} {Variational} {Autoencoders} and {Probabilistic} {Ladder} {Networks}},
	isbn = {978-1-59593-383-6},
	abstract = {Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the ﬂexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the ﬁrst time allowing to train deep models of up to ﬁve stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.},
	language = {en},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	collaborator = {Cohen, William and Moore, Andrew},
	year = {2006},
	file = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks:/Users/mag/Zotero/storage/RQXJUUHM/Cohen and Moore - 2006 - [No title found].pdf:application/pdf},
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2021-02-08},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/W2Z6W9V8/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/8IG4KTLQ/1312.html:text/html},
}

@article{fraccaro_sequential_2016,
	title = {Sequential {Neural} {Models} with {Stochastic} {Layers}},
	url = {http://arxiv.org/abs/1605.07571},
	abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	urldate = {2021-02-08},
	journal = {arXiv:1605.07571 [cs, stat]},
	author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.07571},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2016},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/WX2R4XAS/Fraccaro et al. - 2016 - Sequential Neural Models with Stochastic Layers.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/6BN9MSCY/1605.html:text/html},
}

@article{vahdat_nvae_2021,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	shorttitle = {{NVAE}},
	url = {http://arxiv.org/abs/2007.03898},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
	urldate = {2021-02-10},
	journal = {arXiv:2007.03898 [cs, stat]},
	author = {Vahdat, Arash and Kautz, Jan},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.03898},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Neural Information Processing Systems (NeurIPS) 2020 (spotlight)},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/5A9NQBC3/Vahdat and Kautz - 2021 - NVAE A Deep Hierarchical Variational Autoencoder.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/W95NGNK8/2007.html:text/html},
}

@article{maaloe_biva_2019,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	shorttitle = {{BIVA}},
	url = {http://arxiv.org/abs/1902.02102},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	urldate = {2021-02-10},
	journal = {arXiv:1902.02102 [cs, stat]},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	month = nov,
	year = {2019},
	note = {arXiv: 1902.02102},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/HYD293X4/Maaløe et al. - 2019 - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/QS5GPNTE/1902.html:text/html},
}

@article{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2021-02-18},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/AKCLPM8K/Chung et al. - 2016 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WLRF76GX/1506.html:text/html},
}

@article{bowman_generating_2016,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	urldate = {2021-02-25},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = may,
	year = {2016},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: First two authors contributed equally. Work was done when all authors were at Google, Inc},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/TXCBKICV/Bowman et al. - 2016 - Generating Sentences from a Continuous Space.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/P29MW32M/1511.html:text/html},
}

@inproceedings{shen_towards_2019,
	address = {Florence, Italy},
	title = {Towards {Generating} {Long} and {Coherent} {Text} with {Multi}-{Level} {Latent} {Variable} {Models}},
	url = {https://www.aclweb.org/anthology/P19-1200},
	doi = {10.18653/v1/P19-1200},
	abstract = {Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.},
	urldate = {2021-02-26},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Dinghan and Celikyilmaz, Asli and Zhang, Yizhe and Chen, Liqun and Wang, Xin and Gao, Jianfeng and Carin, Lawrence},
	month = jul,
	year = {2019},
	pages = {2079--2089},
	file = {Full Text PDF:/Users/mag/Zotero/storage/PZ94ITU9/Shen et al. - 2019 - Towards Generating Long and Coherent Text with Mul.pdf:application/pdf},
}

@article{gm_comprehensive_2020,
	title = {A comprehensive survey and analysis of generative models in machine learning},
	volume = {38},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
	doi = {10.1016/j.cosrev.2020.100285},
	abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
	language = {en},
	urldate = {2021-03-18},
	journal = {Computer Science Review},
	author = {Gm, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
	month = nov,
	year = {2020},
	keywords = {Machine learning, Neural networks, Bayesian inference, Deep learning, Generative models},
	pages = {100285},
	file = {ScienceDirect Snapshot:/Users/mag/Zotero/storage/D8M6X482/S1574013720303853.html:text/html},
}

@article{kalatzis_variational_2020,
	title = {Variational {Autoencoders} with {Riemannian} {Brownian} {Motion} {Priors}},
	url = {http://arxiv.org/abs/2002.05227},
	abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
	urldate = {2021-03-24},
	journal = {arXiv:2002.05227 [cs, stat]},
	author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, Søren},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.05227},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in ICML 2020},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/FPIWJRGW/Kalatzis et al. - 2020 - Variational Autoencoders with Riemannian Brownian .pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/GZYWBACL/2002.html:text/html},
}

@article{havtorn_hierarchical_2021,
	title = {Hierarchical {VAEs} {Know} {What} {They} {Don}'t {Know}},
	url = {http://arxiv.org/abs/2102.08248},
	abstract = {Deep generative models have shown themselves to be state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.},
	urldate = {2021-04-08},
	journal = {arXiv:2102.08248 [cs, stat]},
	author = {Havtorn, Jakob D. and Frellsen, Jes and Hauberg, Søren and Maaløe, Lars},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.08248},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 18 pages, source code available at https://github.com/vlievin/biva-pytorch and https://github.com/larsmaaloee/BIVA},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/8PRX4MMP/Havtorn et al. - 2021 - Hierarchical VAEs Know What They Don't Know.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/VPPCAVE5/2102.html:text/html},
}

@article{nalisnick_deep_2019,
	title = {Do {Deep} {Generative} {Models} {Know} {What} {They} {Don}'t {Know}?},
	url = {http://arxiv.org/abs/1810.09136},
	abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
	urldate = {2021-04-08},
	journal = {arXiv:1810.09136 [cs, stat]},
	author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.09136},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2019},
	annote = {Quote:



In this paper, we investigate if modern deep generative models can be used for anomaly detection, as suggested by Bishop (1994) and the AABI pannel (Blei et al., 2017), expecting a well-calibrated model to assign higher density to the training data than to some other data set. However, we find this to not be the case: when trained on CIFAR-10 (Krizhevsky \& Hinton, 2009), VAEs, autoregressive models, and flow-based generative models all assign a higher density to SVHN (Netzer et al., 2011) than to the training data 
 
 
Use this to include and reason that deep generative models DO know (Cite Havtorn)
 


},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/G4JZZEZY/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/9V4FQ2DA/1810.html:text/html},
}

@article{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	shorttitle = {The challenge of realistic music generation},
	url = {http://arxiv.org/abs/1806.10474},
	abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
	urldate = {2021-04-08},
	journal = {arXiv:1806.10474 [cs, eess, stat]},
	author = {Dieleman, Sander and Oord, Aäron van den and Simonyan, Karen},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.10474},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 13 pages, 2 figures, submitted to NIPS 2018},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/4ZT5VP2U/Dieleman et al. - 2018 - The challenge of realistic music generation model.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/Q6DZ2SAX/1806.html:text/html},
}

@article{sutskever_towards_2015,
	title = {Towards {Principled} {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1511.06440},
	abstract = {General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.},
	urldate = {2021-04-16},
	journal = {arXiv:1511.06440 [cs]},
	author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.06440},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/QW5I288U/Sutskever et al. - 2015 - Towards Principled Unsupervised Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/PDM8W7CK/1511.html:text/html},
}

@article{saxena_clockwork_2021,
	title = {Clockwork {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2102.09532},
	abstract = {Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding long-term dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CW-VAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.},
	urldate = {2021-04-19},
	journal = {arXiv:2102.09532 [cs]},
	author = {Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09532},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 17 pages, 12 figures, 4 tables},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/KWTEQ688/Saxena et al. - 2021 - Clockwork Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/LTFWRENV/2102.html:text/html},
}

@article{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2021-04-19},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/EIPH4MZD/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/RQLHXFH7/1711.html:text/html},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2021-04-19},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/CIAZ2DYL/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/CIFEDG2G/1609.html:text/html},
}

@article{kim_variational_2019,
	title = {Variational {Temporal} {Abstraction}},
	url = {http://arxiv.org/abs/1910.00775},
	abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpy-imagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
	urldate = {2021-04-28},
	journal = {arXiv:1910.00775 [cs, stat]},
	author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00775},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted in NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/XUG9RFJ5/Kim et al. - 2019 - Variational Temporal Abstraction.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/GHYZJGCK/1910.html:text/html},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2021-04-28},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/LR4WIS4Y/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/7LG95R9Q/1511.html:text/html},
}

@article{chung_hierarchical_2017,
	title = {Hierarchical {Multiscale} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1609.01704},
	abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
	urldate = {2021-05-03},
	journal = {arXiv:1609.01704 [cs]},
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	month = mar,
	year = {2017},
	note = {arXiv: 1609.01704},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/JTXRDG49/Chung et al. - 2017 - Hierarchical Multiscale Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/MYC2T23W/1609.html:text/html},
}

@article{koutnik_clockwork_2014,
	title = {A {Clockwork} {RNN}},
	url = {http://arxiv.org/abs/1402.3511},
	abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
	urldate = {2021-05-03},
	journal = {arXiv:1402.3511 [cs]},
	author = {Koutník, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Jürgen},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.3511},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/BE2BSCII/Koutník et al. - 2014 - A Clockwork RNN.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/FR25Q8DW/1402.html:text/html},
}

@article{hafner_learning_2019,
	title = {Learning {Latent} {Dynamics} for {Planning} from {Pixels}},
	url = {http://arxiv.org/abs/1811.04551},
	abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
	urldate = {2021-05-03},
	journal = {arXiv:1811.04551 [cs, stat]},
	author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.04551},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 20 pages, 12 figures, 1 table},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/UVG7MWA6/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/ZAVJU2F2/1811.html:text/html},
}

@article{auger-methe_state-space_2016,
	title = {State-space models’ dirty little secrets: even simple linear {Gaussian} models can have estimation problems},
	volume = {6},
	copyright = {2016 The Author(s)},
	issn = {2045-2322},
	shorttitle = {State-space models’ dirty little secrets},
	url = {https://www.nature.com/articles/srep26677},
	doi = {10.1038/srep26677},
	abstract = {State-space models (SSMs) are increasingly used in ecology to model time-series such as animal movement paths and population dynamics. This type of hierarchical model is often structured to account for two levels of variability: biological stochasticity and measurement error. SSMs are flexible. They can model linear and nonlinear processes using a variety of statistical distributions. Recent ecological SSMs are often complex, with a large number of parameters to estimate. Through a simulation study, we show that even simple linear Gaussian SSMs can suffer from parameter- and state-estimation problems. We demonstrate that these problems occur primarily when measurement error is larger than biological stochasticity, the condition that often drives ecologists to use SSMs. Using an animal movement example, we show how these estimation problems can affect ecological inference. Biased parameter estimates of a SSM describing the movement of polar bears (Ursus maritimus) result in overestimating their energy expenditure. We suggest potential solutions, but show that it often remains difficult to estimate parameters. While SSMs are powerful tools, they can give misleading results and we urge ecologists to assess whether the parameters can be estimated accurately before drawing ecological conclusions from their results.},
	language = {en},
	number = {1},
	urldate = {2021-05-03},
	journal = {Scientific Reports},
	author = {Auger-Méthé, Marie and Field, Chris and Albertsen, Christoffer M. and Derocher, Andrew E. and Lewis, Mark A. and Jonsen, Ian D. and Mills Flemming, Joanna},
	month = may,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {26677},
	file = {Full Text PDF:/Users/mag/Zotero/storage/KVPDMQVG/Auger-Méthé et al. - 2016 - State-space models’ dirty little secrets even sim.pdf:application/pdf;Snapshot:/Users/mag/Zotero/storage/6CJJKJ5Q/srep26677.html:text/html},
}

@article{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2021-05-03},
	journal = {arXiv:2006.11239 [cs, stat]},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.11239},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/ZL4PVKKW/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/9GHB49N7/2006.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-05-05},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/VACAPZGJ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/CDR34UHF/1706.html:text/html},
}

@inproceedings{sundermeyer_lstm_2012,
	title = {{LSTM} neural networks for language modeling},
	abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a ﬁxed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difﬁcult to train and therefore are unlikely to show the full potential of recurrent models.},
	language = {en},
	booktitle = {Thirteenth annual conference of the international speech communication association},
	author = {Sundermeyer, Martin and Schlüter, Ralf and Ney, Hermann},
	year = {2012},
	pages = {4},
	file = {Sundermeyer et al. - LSTM Neural Networks for Language Modeling.pdf:/Users/mag/Zotero/storage/TI459Z6R/Sundermeyer et al. - LSTM Neural Networks for Language Modeling.pdf:application/pdf},
}

@article{williams_hierarchical_2020,
	title = {Hierarchical {Quantized} {Autoencoders}},
	url = {http://arxiv.org/abs/2002.08111},
	abstract = {Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.},
	urldate = {2021-05-11},
	journal = {arXiv:2002.08111 [cs, stat]},
	author = {Williams, Will and Ringer, Sam and Ash, Tom and Hughes, John and MacLeod, David and Dougherty, Jamie},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.08111},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/Y4CD8T25/Williams et al. - 2020 - Hierarchical Quantized Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WXDU7FB8/2002.html:text/html},
}

@article{dai_transformer-xl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2021-05-12},
	journal = {arXiv:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ACL 2019 long paper. Code and pretrained models are available at https://github.com/kimiyoung/transformer-xl},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/9R36IKZ9/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/2UHHPIKE/1901.html:text/html},
}

@article{stoller_seq-u-net_2019,
	title = {Seq-{U}-{Net}: {A} {One}-{Dimensional} {Causal} {U}-{Net} for {Efficient} {Sequence} {Modelling}},
	shorttitle = {Seq-{U}-{Net}},
	url = {http://arxiv.org/abs/1911.06393},
	abstract = {Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. However, efficiently modelling long-term dependencies in these sequences is still challenging. Although the receptive field of these models grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, prohibiting the use of longer receptive fields in practice. To increase efficiency, we make use of the "slow feature" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model ("Seq-U-Net") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance in all tasks.},
	urldate = {2021-05-16},
	journal = {arXiv:1911.06393 [cs, eess, stat]},
	author = {Stoller, Daniel and Tian, Mi and Ewert, Sebastian and Dixon, Simon},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06393},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Code available at https://github.com/f90/Seq-U-Net},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/TWZGKCEZ/Stoller et al. - 2019 - Seq-U-Net A One-Dimensional Causal U-Net for Effi.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/I93HQQ4K/1911.html:text/html},
}

@article{hono_periodnet_2021,
	title = {{PeriodNet}: {A} non-autoregressive waveform generation model with a structure separating periodic and aperiodic components},
	shorttitle = {{PeriodNet}},
	url = {http://arxiv.org/abs/2102.07786},
	abstract = {We propose PeriodNet, a non-autoregressive (non-AR) waveform generation model with a new model structure for modeling periodic and aperiodic components in speech waveforms. The non-AR waveform generation models can generate speech waveforms parallelly and can be used as a speech vocoder by conditioning an acoustic feature. Since a speech waveform contains periodic and aperiodic components, both components should be appropriately modeled to generate a high-quality speech waveform. However, it is difficult to decompose the components from a natural speech waveform in advance. To address this issue, we propose a parallel model and a series model structure separating periodic and aperiodic components. The features of our proposed models are that explicit periodic and aperiodic signals are taken as input, and external periodic/aperiodic decomposition is not needed in training. Experiments using a singing voice corpus show that our proposed structure improves the naturalness of the generated waveform. We also show that the speech waveforms with a pitch outside of the training data range can be generated with more naturalness.},
	urldate = {2021-05-16},
	journal = {arXiv:2102.07786 [cs, eess]},
	author = {Hono, Yukiya and Takaki, Shinji and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.07786},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: 5 pages, accepted to ICASSP 2021},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/JZQQM82T/Hono et al. - 2021 - PeriodNet A non-autoregressive waveform generatio.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WUE4A5JR/2102.html:text/html},
}

@article{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	shorttitle = {Tacotron},
	url = {/paper/Tacotron%3A-Towards-End-to-End-Speech-Synthesis-Wang-Skerry-Ryan/a072c2a400f62f720b68dc54a662fb1ae115bf06},
	abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it\&\#39;s substantially faster than sample-level autoregressive methods.},
	language = {en},
	urldate = {2021-05-16},
	journal = {undefined},
	author = {Wang, Yuxuan and Skerry-Ryan, R. and Stanton, Daisy and Wu, Y. and Weiss, Ron J. and Jaitly, Navdeep and Yang, Z. and Xiao, Y. and Chen, Z. and Bengio, S. and Le, Quoc V. and Agiomyrgiannakis, Yannis and Clark, R. and Saurous, R.},
	year = {2017},
	file = {Full Text PDF:/Users/mag/Zotero/storage/ANWGN2FM/Wang et al. - 2017 - Tacotron Towards End-to-End Speech Synthesis.pdf:application/pdf;Snapshot:/Users/mag/Zotero/storage/VUE4WA9F/a072c2a400f62f720b68dc54a662fb1ae115bf06.html:text/html},
}

@article{elias_parallel_2021,
	title = {Parallel {Tacotron} 2: {A} {Non}-{Autoregressive} {Neural} {TTS} {Model} with {Differentiable} {Duration} {Modeling}},
	shorttitle = {Parallel {Tacotron} 2},
	url = {http://arxiv.org/abs/2103.14574},
	abstract = {This paper introduces Parallel Tacotron 2, a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic Time Warping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations. Its duration control capability is also demonstrated.},
	urldate = {2021-05-16},
	journal = {arXiv:2103.14574 [cs, eess]},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Skerry-Ryan, R. J. and Wu, Yonghui},
	month = apr,
	year = {2021},
	note = {arXiv: 2103.14574},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Submitted to INTERSPEECH 2021},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/L3QA8X4P/Elias et al. - 2021 - Parallel Tacotron 2 A Non-Autoregressive Neural T.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/GJZ7QD2H/2103.html:text/html},
}

@article{yu_multi-scale_2016,
	title = {Multi-{Scale} {Context} {Aggregation} by {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1511.07122},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	urldate = {2021-05-17},
	journal = {arXiv:1511.07122 [cs]},
	author = {Yu, Fisher and Koltun, Vladlen},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.07122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published as a conference paper at ICLR 2016},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/IA8TNTA5/Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolu.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/3P939YP9/1511.html:text/html},
}

@article{oord_parallel_2017,
	title = {Parallel {WaveNet}: {Fast} {High}-{Fidelity} {Speech} {Synthesis}},
	shorttitle = {Parallel {WaveNet}},
	url = {http://arxiv.org/abs/1711.10433},
	abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
	urldate = {2021-05-17},
	journal = {arXiv:1711.10433 [cs]},
	author = {Oord, Aaron van den and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George van den and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10433},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/9R4QDKYA/Oord et al. - 2017 - Parallel WaveNet Fast High-Fidelity Speech Synthe.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/D9AJJH49/1711.html:text/html},
}

@article{paine_fast_2016,
	title = {Fast {Wavenet} {Generation} {Algorithm}},
	url = {http://arxiv.org/abs/1611.09482},
	abstract = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2{\textasciicircum}L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
	urldate = {2021-05-17},
	journal = {arXiv:1611.09482 [cs]},
	author = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.09482},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Comment: Technical Report},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/XUHV68SR/Paine et al. - 2016 - Fast Wavenet Generation Algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/XLU3ZVS6/1611.html:text/html},
}
