
@article{maaloe_biva_nodate,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, ﬂow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classiﬁcation tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	language = {en},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	pages = {12},
	file = {BIVA_camera_ready_appendix_v2.pdf:/Users/mag/Zotero/storage/F9XHSNDZ/BIVA_camera_ready_appendix_v2.pdf:application/pdf;Maaløe et al. - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:/Users/mag/Zotero/storage/GCR945VW/Maaløe et al. - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:application/pdf},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2020-12-03},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = aug,
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/7CVKYWTV/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WEU4N3RE/1606.html:text/html},
}

@inproceedings{fraccaro_generative_2018,
	title = {Generative {Temporal} {Models} with {Spatial} {Memory} for {Partially} {Observed} {Environments}},
	url = {http://proceedings.mlr.press/v80/fraccaro18a.html},
	abstract = {In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent’s representations during training or vi...},
	language = {en},
	urldate = {2021-02-04},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fraccaro, Marco and Rezende, Danilo and Zwols, Yori and Pritzel, Alexander and Eslami, S. M. Ali and Viola, Fabio},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1549--1558},
	file = {Snapshot:/Users/mag/Zotero/storage/VTFL7F58/fraccaro18a.html:text/html;Full Text PDF:/Users/mag/Zotero/storage/8MZXF9TZ/Fraccaro et al. - 2018 - Generative Temporal Models with Spatial Memory for.pdf:application/pdf},
}

@article{gemici_generative_2017,
	title = {Generative {Temporal} {Models} with {Memory}},
	url = {http://arxiv.org/abs/1702.04649},
	abstract = {We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.},
	urldate = {2021-02-04},
	journal = {arXiv:1702.04649 [cs, stat]},
	author = {Gemici, Mevlana and Hung, Chia-Chun and Santoro, Adam and Wayne, Greg and Mohamed, Shakir and Rezende, Danilo J. and Amos, David and Lillicrap, Timothy},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04649},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/LV2BVV5F/Gemici et al. - 2017 - Generative Temporal Models with Memory.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/5ZYYIMHG/1702.html:text/html},
}

@article{sonderby_ladder_2016,
	title = {Ladder {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1602.02282},
	abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
	urldate = {2021-02-07},
	journal = {arXiv:1602.02282 [cs, stat]},
	author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
	month = may,
	year = {2016},
	note = {arXiv: 1602.02282},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/JAEZIP5W/Sønderby et al. - 2016 - Ladder Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/62GGTJTI/1602.html:text/html},
}

@inproceedings{cohen_how_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {How to {Train} {Deep} {Variational} {Autoencoders} and {Probabilistic} {Ladder} {Networks}},
	isbn = {978-1-59593-383-6},
	abstract = {Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the ﬂexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the ﬁrst time allowing to train deep models of up to ﬁve stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.},
	language = {en},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	collaborator = {Cohen, William and Moore, Andrew},
	year = {2006},
	file = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks:/Users/mag/Zotero/storage/RQXJUUHM/Cohen and Moore - 2006 - [No title found].pdf:application/pdf},
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2021-02-08},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/W2Z6W9V8/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/8IG4KTLQ/1312.html:text/html},
}

@article{fraccaro_sequential_2016,
	title = {Sequential {Neural} {Models} with {Stochastic} {Layers}},
	url = {http://arxiv.org/abs/1605.07571},
	abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	urldate = {2021-02-08},
	journal = {arXiv:1605.07571 [cs, stat]},
	author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.07571},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2016},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/WX2R4XAS/Fraccaro et al. - 2016 - Sequential Neural Models with Stochastic Layers.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/6BN9MSCY/1605.html:text/html},
}

@article{vahdat_nvae_2021,
	title = {{NVAE}: {A} {Deep} {Hierarchical} {Variational} {Autoencoder}},
	shorttitle = {{NVAE}},
	url = {http://arxiv.org/abs/2007.03898},
	abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\${\textbackslash}times\$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
	urldate = {2021-02-10},
	journal = {arXiv:2007.03898 [cs, stat]},
	author = {Vahdat, Arash and Kautz, Jan},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.03898},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Neural Information Processing Systems (NeurIPS) 2020 (spotlight)},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/5A9NQBC3/Vahdat and Kautz - 2021 - NVAE A Deep Hierarchical Variational Autoencoder.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/W95NGNK8/2007.html:text/html},
}

@article{maaloe_biva_2019,
	title = {{BIVA}: {A} {Very} {Deep} {Hierarchy} of {Latent} {Variables} for {Generative} {Modeling}},
	shorttitle = {{BIVA}},
	url = {http://arxiv.org/abs/1902.02102},
	abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
	urldate = {2021-02-10},
	journal = {arXiv:1902.02102 [cs, stat]},
	author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
	month = nov,
	year = {2019},
	note = {arXiv: 1902.02102},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/HYD293X4/Maaløe et al. - 2019 - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/QS5GPNTE/1902.html:text/html},
}

@article{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2021-02-18},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/AKCLPM8K/Chung et al. - 2016 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/WLRF76GX/1506.html:text/html},
}

@article{bowman_generating_2016,
	title = {Generating {Sentences} from a {Continuous} {Space}},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	urldate = {2021-02-25},
	journal = {arXiv:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	month = may,
	year = {2016},
	note = {arXiv: 1511.06349},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: First two authors contributed equally. Work was done when all authors were at Google, Inc},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/TXCBKICV/Bowman et al. - 2016 - Generating Sentences from a Continuous Space.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/P29MW32M/1511.html:text/html},
}

@article{chung_recurrent_2016-1,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2021-02-25},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/FCIRR3N6/Chung et al. - 2016 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/BJIHVG4W/1506.html:text/html},
}

@inproceedings{shen_towards_2019,
	address = {Florence, Italy},
	title = {Towards {Generating} {Long} and {Coherent} {Text} with {Multi}-{Level} {Latent} {Variable} {Models}},
	url = {https://www.aclweb.org/anthology/P19-1200},
	doi = {10.18653/v1/P19-1200},
	abstract = {Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation with latent variables. However, previous works typically focus on synthesizing relatively short sentences (up to 20 words), and the posterior collapse issue has been widely identified in text-VAEs. In this paper, we propose to leverage several multi-level structures to learn a VAE model for generating long, and coherent text. In particular, a hierarchy of stochastic layers between the encoder and decoder networks is employed to abstract more informative and semantic-rich latent codes. Besides, we utilize a multi-level decoder structure to capture the coherent long-term structure inherent in long-form texts, by generating intermediate sentence representations as high-level plan vectors. Extensive experimental results demonstrate that the proposed multi-level VAE model produces more coherent and less repetitive long text compared to baselines as well as can mitigate the posterior-collapse issue.},
	urldate = {2021-02-26},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Dinghan and Celikyilmaz, Asli and Zhang, Yizhe and Chen, Liqun and Wang, Xin and Gao, Jianfeng and Carin, Lawrence},
	month = jul,
	year = {2019},
	pages = {2079--2089},
	file = {Full Text PDF:/Users/mag/Zotero/storage/PZ94ITU9/Shen et al. - 2019 - Towards Generating Long and Coherent Text with Mul.pdf:application/pdf},
}

@article{gm_comprehensive_2020,
	title = {A comprehensive survey and analysis of generative models in machine learning},
	volume = {38},
	issn = {1574-0137},
	url = {https://www.sciencedirect.com/science/article/pii/S1574013720303853},
	doi = {10.1016/j.cosrev.2020.100285},
	abstract = {Generative models have been in existence for many decades. In the field of machine learning, we come across many scenarios when directly learning a target is intractable through discriminative models, and in such cases the joint distribution of the target and the training data is approximated and generated. These generative models help us better represent or model a set of data by generating data in the form of Markov chains or simply employing a generative iterative process to do the same. With the recent innovation of Generative Adversarial Networks (GANs), it is now possible to make use of AI to generate pieces of art, music, etc. with a high extent of realism. In this paper, we review and analyse critically all the generative models, namely Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), Latent Dirichlet Allocation (LDA), Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN), Deep Boltzmann Machines (DBM), and GANs. We study their algorithms and implement each of the models to provide the reader some insights on which generative model to pick from while dealing with a problem. We also provide some noteworthy contributions done in the past to these models from the literature.},
	language = {en},
	urldate = {2021-03-18},
	journal = {Computer Science Review},
	author = {Gm, Harshvardhan and Gourisaria, Mahendra Kumar and Pandey, Manjusha and Rautaray, Siddharth Swarup},
	month = nov,
	year = {2020},
	keywords = {Machine learning, Neural networks, Bayesian inference, Deep learning, Generative models},
	pages = {100285},
	file = {ScienceDirect Snapshot:/Users/mag/Zotero/storage/D8M6X482/S1574013720303853.html:text/html},
}

@article{kalatzis_variational_2020,
	title = {Variational {Autoencoders} with {Riemannian} {Brownian} {Motion} {Priors}},
	url = {http://arxiv.org/abs/2002.05227},
	abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
	urldate = {2021-03-24},
	journal = {arXiv:2002.05227 [cs, stat]},
	author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, Søren},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.05227},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in ICML 2020},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/FPIWJRGW/Kalatzis et al. - 2020 - Variational Autoencoders with Riemannian Brownian .pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/GZYWBACL/2002.html:text/html},
}

@article{havtorn_hierarchical_2021,
	title = {Hierarchical {VAEs} {Know} {What} {They} {Don}'t {Know}},
	url = {http://arxiv.org/abs/2102.08248},
	abstract = {Deep generative models have shown themselves to be state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.},
	urldate = {2021-04-08},
	journal = {arXiv:2102.08248 [cs, stat]},
	author = {Havtorn, Jakob D. and Frellsen, Jes and Hauberg, Søren and Maaløe, Lars},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.08248},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 18 pages, source code available at https://github.com/vlievin/biva-pytorch and https://github.com/larsmaaloee/BIVA},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/8PRX4MMP/Havtorn et al. - 2021 - Hierarchical VAEs Know What They Don't Know.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/VPPCAVE5/2102.html:text/html},
}

@article{nalisnick_deep_2019,
	title = {Do {Deep} {Generative} {Models} {Know} {What} {They} {Don}'t {Know}?},
	url = {http://arxiv.org/abs/1810.09136},
	abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
	urldate = {2021-04-08},
	journal = {arXiv:1810.09136 [cs, stat]},
	author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.09136},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2019},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/G4JZZEZY/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/9V4FQ2DA/1810.html:text/html},
}

@article{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	shorttitle = {The challenge of realistic music generation},
	url = {http://arxiv.org/abs/1806.10474},
	abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
	urldate = {2021-04-08},
	journal = {arXiv:1806.10474 [cs, eess, stat]},
	author = {Dieleman, Sander and Oord, Aäron van den and Simonyan, Karen},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.10474},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 13 pages, 2 figures, submitted to NIPS 2018},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/4ZT5VP2U/Dieleman et al. - 2018 - The challenge of realistic music generation model.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/Q6DZ2SAX/1806.html:text/html},
}

@article{sutskever_towards_2015,
	title = {Towards {Principled} {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1511.06440},
	abstract = {General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks. In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain.},
	urldate = {2021-04-16},
	journal = {arXiv:1511.06440 [cs]},
	author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.06440},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/QW5I288U/Sutskever et al. - 2015 - Towards Principled Unsupervised Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/PDM8W7CK/1511.html:text/html},
}

@article{saxena_clockwork_2021,
	title = {Clockwork {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2102.09532},
	abstract = {Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding long-term dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CW-VAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.},
	urldate = {2021-04-19},
	journal = {arXiv:2102.09532 [cs]},
	author = {Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09532},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 17 pages, 12 figures, 4 tables},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/KWTEQ688/Saxena et al. - 2021 - Clockwork Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/LTFWRENV/2102.html:text/html},
}

@article{oord_neural_2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2021-04-19},
	journal = {arXiv:1711.00937 [cs]},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv: 1711.00937},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/EIPH4MZD/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/RQLHXFH7/1711.html:text/html},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2021-04-19},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/mag/Zotero/storage/CIAZ2DYL/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/mag/Zotero/storage/CIFEDG2G/1609.html:text/html},
}
